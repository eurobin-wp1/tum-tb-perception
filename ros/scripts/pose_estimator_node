#!/usr/bin/env python3

"""
Estimates the poses of objects from image bounding boxes (BBs) and depth data.

Note: this component was designed for and tested with an Intel Realsense D435
sensor. In particular, it expects messages to be published on the following 
topics (configurable through launch parameters):
    - /camera/color/image_raw (Image)
    - /camera/depth/color/points (PointCloud2)
    - /camera/color/camera_info  (CameraInfo)
"""

import os
import sys
import time
import copy
import pickle
import socket

import rospy
import rospkg
import tf2_ros
import tf_conversions
import numpy as np
import sensor_msgs.point_cloud2 as pc2
from sklearn.cluster import AgglomerativeClustering

from geometry_msgs.msg import Point, Pose, Quaternion, Vector3, TransformStamped
from sensor_msgs.msg import PointCloud, PointCloud2, CameraInfo
from visualization_msgs.msg import Marker, MarkerArray
from eurobin_perception.msg import BoundingBoxList, ObjectList, Object

from eurobin_perception.pose_estimation import (
    PositionEstimator,
    remove_outliers_agglomerative,
)
from eurobin_perception.utils import bbox_list_msg_to_list, obj_list_msg_to_json
from eurobin_perception.visualization import load_class_color_map

# For debug visualizations:
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from eurobin_perception.utils import _arrow3D, _annotate3D, minimum_bounding_rectangle, rotation_matrix_from_vectors
setattr(Axes3D, 'arrow3D', _arrow3D)
setattr(Axes3D, 'annotate3D', _annotate3D)


## ----------------------------------------------------------------------
## ROS Callbacks and Message Initializations:
## ----------------------------------------------------------------------

current_pc2_msg_ = None
current_detection_msg_ = None
current_camera_info_msg_ = None

def pc2_callback(msg):
    global current_pc2_msg_ 
    current_pc2_msg_ = msg

def detection_callback(msg):
    global current_detection_msg_ 
    current_detection_msg_ = msg

def camera_info_callback(msg):
    global current_camera_info_msg_ 
    current_camera_info_msg_ = msg

object_marker_publisher_ = None

## ----------------------------------------------------------------------
## Functions:
## ----------------------------------------------------------------------

def get_point_markers(point, frame_id, label='', color_value=(0., 0., 0.)):
    """
    Creates RViz position and text markers for a given 3D point.

    Parameters
    ----------
    point: ndarray
        3D position coordinates
    frame_id: str
        Name of given point's coordinate frame
    label: str
        Text with which the point will be labeled
    color_value: tuple
       (R, G, B) values for the point and text markers [0., 255.] 

    Returns
    -------
    marker_msg: visualization_msgs.Marker 
        Point visualization marker ROS message
    text_marker_msg: visualization_msgs.Marker 
        Text visualization marker ROS message
    """
    global object_marker_id_

    marker_msg = Marker()
    marker_msg.header.frame_id = frame_id
    marker_msg.id = object_marker_id_
    marker_msg.type = 2                                       # Sphere
    marker_msg.action = 0                                     # Add/modify
    marker_msg.pose = Pose(position=Point(*point), 
                           orientation=Quaternion(0., 0., 0., 1.))
    marker_msg.scale.x = 0.01
    marker_msg.scale.y = 0.01
    marker_msg.scale.z = 0.01
    marker_msg.color.r = color_value[0] / 255.
    marker_msg.color.g = color_value[1] / 255.
    marker_msg.color.b = color_value[2] / 255.
    marker_msg.color.a = 1.0
    object_marker_id_ += 1

    text_marker_msg = copy.deepcopy(marker_msg)
    text_marker_msg.id = object_marker_id_
    text_marker_msg.type = 9                                  # Text-view-facing
    text_marker_msg.text = label
    text_marker_msg.pose.position.x = marker_msg.pose.position.x + \
                                      (0.005 * (len(label) / 2.))
    text_marker_msg.pose.position.y = marker_msg.pose.position.y - 0.01
    object_marker_id_ += 1

    return marker_msg, text_marker_msg

def clear_object_markers():
    """
    Clears all current RViz object markers.

    Parameters
    -------
    None

    Returns
    -------
    None
    """
    global object_marker_id_, object_marker_publisher_

    marker_array_msg = MarkerArray()

    marker_msg = Marker()
    marker_msg.id = object_marker_id_ 
    marker_msg.action = 3                                   # Delete all

    marker_array_msg.markers.append(marker_msg)
    object_marker_publisher_.publish(marker_array_msg)

    # Reset object marker ID:
    object_marker_id_ = 0

def get_camera_params_dict():
    """
    Retrieves and returns camera parameters in a dict.

    Parameters
    -------
    None

    Returns
    -------
    camera_params_dict: dict
        Camera intrinsic parameters (P matrix elements)
    """
    global current_camera_info_msg_ 

    f_x = current_camera_info_msg_.P[0]; f_y = current_camera_info_msg_.P[5]
    c_x = current_camera_info_msg_.P[2]; c_y = current_camera_info_msg_.P[6]

    return {'f_x': f_x, 'f_y': f_y, 'c_x': c_x, 'c_y': c_y}


if __name__ == '__main__':
    ## ----------------------------------------------------------------------
    ## ROS Initializations:
    ## ----------------------------------------------------------------------
    rospy.init_node('pose_estimator')
    rate = rospy.Rate(10)

    package_path = rospkg.RosPack().get_path('eurobin_perception')

    class_colors_file_path = rospy.get_param('~class_colors_file_path', '')
    output_dir_path = rospy.get_param('~output_dir_path', 
                                      os.path.join(package_path, 'output_data'))
    taskboard_frame_name = rospy.get_param('~taskboard_frame_name', 'taskboard_frame')
    num_retries = rospy.get_param('~num_retries', 3)
    udp_ip = rospy.get_param('~udp_ip', 'localhost')
    udp_output_port = rospy.get_param('~udp_output_port', 6000)

    pointcloud_topic = rospy.get_param('~pointcloud_topic', '/camera/depth/color/points')
    camera_info_topic = rospy.get_param('~camera_info_topic', '/camera/color/camera_info')
    detector_result_topic = rospy.get_param('~detector_result_topic', '/eurobin_perception/detection_result')
    object_positions_pub_topic = rospy.get_param('~object_positions_pub_topic', '/eurobin_perception/object_positions')
    object_poses_pub_topic = rospy.get_param('~object_poses_pub_topic', '/eurobin_perception/object_poses')
    object_marker_pub_topic = rospy.get_param('~object_marker_pub_topic', '/eurobin_perception/object_markers')
    cropped_pc_pub_topic = rospy.get_param('~cropped_pc_pub_topic', '/eurobin_perception/cropped_pc')

    debug = rospy.get_param('~debug', False)
    save_output = rospy.get_param('~save_output', False)

    pc2_subscriber = rospy.Subscriber(pointcloud_topic, PointCloud2, pc2_callback)
    detector_result_subscriber = rospy.Subscriber(detector_result_topic, BoundingBoxList, detection_callback)
    camera_info_subscriber = rospy.Subscriber(camera_info_topic, CameraInfo, camera_info_callback)

    object_positions_publisher = rospy.Publisher(object_positions_pub_topic, ObjectList, queue_size=10)
    object_poses_publisher = rospy.Publisher(object_poses_pub_topic, ObjectList, queue_size=10)
    object_marker_publisher_ = rospy.Publisher(object_marker_pub_topic, MarkerArray, queue_size=10)
    if debug:
        cropped_pc_debug_publisher = rospy.Publisher(cropped_pc_pub_topic, PointCloud, queue_size=10)

    tf_broadcaster = tf2_ros.TransformBroadcaster()

    ## ----------------------------------------------------------------------
    ## UDP Initializations
    ## ----------------------------------------------------------------------

    rospy.loginfo(f'[pose_estimator] Initializing UDP socket with address' + \
                  f' family AF_INET and type SOCK_DGRAM')
    rospy.loginfo(f'[pose_estimator] Will send messages over IP {udp_ip}' + \
                  f' and port {udp_output_port}.')
    udp_output_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

    ## ----------------------------------------------------------------------
    ## Estimator Execution:
    ## ----------------------------------------------------------------------

    rospy.loginfo('[pose_estimator] Waiting for first camera info message on topic: {}'.format(camera_info_topic))
    while current_camera_info_msg_ is None:
        rospy.sleep(0.1)

        if rospy.is_shutdown():
            rospy.loginfo('[pose_estimator] Stopping node...')
            sys.exit(0)

    class_colors_dict = load_class_color_map(class_colors_file_path)
    orientation_quaternion = None
    object_marker_id_ = 0

    # Initialize PositionEstimator (with current camera parameters):
    position_estimator = PositionEstimator(camera_params_dict=get_camera_params_dict())

    rospy.loginfo('[pose_estimator] Will estimate object poses for every message received on topic: {}'.format(detector_result_topic))
    try:
        while not rospy.is_shutdown():
            if current_detection_msg_ is not None:
                clear_object_markers()
                if current_pc2_msg_ is not None:

                    rospy.loginfo('[pose_estimator] Received detection result message.')
                    rospy.loginfo('[pose_estimator] Estimating detected object positions...')
                    position_estimation_start_time = time.time()

                    # Extract list of point positions (x, y, z) sensor_msgs/PointCloud2 message:
                    read_points_start_time = time.time()
                    pc_point_list = pc2.read_points_list(current_pc2_msg_, 
                                                         skip_nans=True, 
                                                         field_names=("x", "y", "z"))
                    if debug:
                        print(f'[DEBUG] [pose_estimator] Converted PC2 msg to points list in {time.time() - read_points_start_time:.2f}s')
                    # TODO: Consider moving to ros_numpy (partial) vectorization if runtime must be improved.

                    bbox_dict_list = bbox_list_msg_to_list(current_detection_msg_)

                    object_positions_dict, object_points_dict, \
                    cropped_pc_points_array = \
                            position_estimator.estimate_object_positions(
                                    bbox_dict_list, pc_point_list, 
                                    cropped_pc_label='taskboard', 
                                    debug=debug
                    )

                    object_list_msg = ObjectList()
                    marker_array_msg = MarkerArray()

                    for label, object_position in object_positions_dict.items():
                        object_msg = Object(label=label, pose=Pose(position=Point(*object_position), 
                                                                   orientation=Quaternion(0., 0., 0., 1.)))
                        object_msg.header.frame_id = current_camera_info_msg_.header.frame_id
                        object_list_msg.objects.append(object_msg)

                        marker_msg, \
                            text_marker_msg = get_point_markers(
                                    object_position,
                                    frame_id=current_camera_info_msg_.header.frame_id,
                                    label=object_msg.label,
                                    color_value=class_colors_dict[object_msg.label]
                        )
                        marker_array_msg.markers.append(marker_msg)
                        marker_array_msg.markers.append(text_marker_msg)

                    object_positions_publisher.publish(object_list_msg)
                    object_marker_publisher_.publish(marker_array_msg)

                    if debug:
                        cropped_pc_msg = PointCloud()
                        cropped_pc_msg.header.frame_id = current_camera_info_msg_.header.frame_id
                        cropped_pc_msg.header.stamp = current_pc2_msg_.header.stamp

                        for point in cropped_pc_points_array:
                            cropped_pc_msg.points.append(Point(x=point[0], y=point[1], z=point[2]))
                        cropped_pc_debug_publisher.publish(cropped_pc_msg)

                    elapsed_time = time.time() - position_estimation_start_time
                    rospy.loginfo(f'[pose_estimator] Estimated object positions in {elapsed_time:.2f}s')

                    # Optionally save results: taskboard points and object positions dict.
                    # Mostly for testing and debugging.
                    if save_output:
                        rospy.loginfo(f'[pose_estimator] Saving output data...')
                        if not os.path.isdir(output_dir_path):
                            rospy.loginfo(f'[pose_estimator] Output directory'
                                          f' {output_dir_path} does not exist!'
                                          f' Creating now...')
                            os.makedirs(output_dir_path)

                        with open(os.path.join(output_dir_path, 'pose_estimator_object_positions_dict.pkl'), 'wb') as handle:
                            pickle.dump(object_positions_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
                        np.save(os.path.join(output_dir_path, 'pose_estimator_taskboard_points.npy'), np.stack(object_points_dict['taskboard']))


                    ## ----------------------------------------------------------------------
                    ## Orientation Estimation Test Program:
                    ## ----------------------------------------------------------------------

                    # Run orientation estimation until successful a maximum of 
                    # num_retries times.
                    for attempt_id in range(num_retries):

                        tb_orientation_estimation_start_time = time.time()

                        ## ----------------------------------------
                        ## Parameters and Variables:
                        ## ----------------------------------------

                        arrow_scale_factor = 0.2
                        hide_pc_points = False
                        plot_fitted_rectangle, plot_fitted_rectified_rectangle = True, True

                        vertical_side_found, horizontal_side_found = False, False

                        ## ----------------------------------------
                        ## Data Loading and Preprocessing:
                        ## ----------------------------------------

                        # Get 3D point cloud segment corresponding to the taskboard:
                        tb_points_array = object_points_dict['taskboard']
                        tb_position = object_positions_dict['taskboard']

                        rospy.loginfo(f'[pose_estimator] Removing TB PC outliers' + \
                                      f' using agglomerative clustering...')
                        tb_points_array = remove_outliers_agglomerative(tb_points_array, debug=debug)

                        ## ----------------------------------------
                        ## Extracting Normal of Best-Fit Plane:
                        ## ----------------------------------------

                        # Get best-fit plane normal:
                        eigenvectors_eigh = np.linalg.eigh(np.cov(tb_points_array.T))[1]
                        plane_normal_eigh = eigenvectors_eigh[:, 0]
                        # If z coordinate is negative, invert the vector to rectify
                        # the resultant axes:
                        if plane_normal_eigh[2] < 0.:
                            plane_normal_eigh = -plane_normal_eigh 

                        ## ----------------------------------------
                        ## Fitting Minimum Bounding Rectangle on Planar Data:
                        ## ----------------------------------------

                        # Estimate minimum bounding rectangle on data projected onto best-fit plane:
                        rot_matrix = rotation_matrix_from_vectors([0, 0, 1], plane_normal_eigh)
                        points_oriented_array = tb_points_array @ rot_matrix

                        rect_oriented_corners = minimum_bounding_rectangle(points_oriented_array[:, :2])
                        mid_point_oriented = points_oriented_array.mean(axis=0)

                        rect_oriented_corners_aug = np.hstack((rect_oriented_corners,
                                                      np.ones((rect_oriented_corners.shape[0], 1)) * mid_point_oriented[2]))
                        rect_rectified_corners = rect_oriented_corners_aug @ rot_matrix.T

                        ## ----------------------------------------
                        ## Estimating Taskboard Orientation:
                        ## ----------------------------------------

                        # Localize quadrant corners using known detected objects:
                        object_nearest_corner_dict = {}
                        for object_id, position_array in object_positions_dict.items():
                            corner_id = np.linalg.norm(rect_rectified_corners - position_array, axis=1).argmin()
                            object_nearest_corner_dict[object_id] = corner_id

                        ## Updated direction estimation algorithm:
                        quadrant_corner_ids = {}
                        if 'red_button' in object_nearest_corner_dict.keys():
                            quadrant_corner_ids['1'] = object_nearest_corner_dict['red_button']
                        elif 'blue_button' in object_nearest_corner_dict.keys():
                            quadrant_corner_ids['1'] = object_nearest_corner_dict['blue_button']
                        elif 'multimeter_connector' in object_nearest_corner_dict.keys():
                            quadrant_corner_ids['1'] = object_nearest_corner_dict['multimeter_connector']
                        else:
                            rospy.logerr('[pose_estimator] Could not locate quadrant 1!')
                            quadrant_corner_ids['1'] = None

                        if 'hatch_handle' in object_nearest_corner_dict.keys():
                            quadrant_corner_ids['2'] = object_nearest_corner_dict['hatch_handle']
                        elif 'multimeter_probe' in object_nearest_corner_dict.keys():
                            quadrant_corner_ids['2'] = object_nearest_corner_dict['multimeter_probe']
                        else:
                            rospy.logerr('[pose_estimator] Could not locate quadrant 2!')
                            quadrant_corner_ids['2'] = None

                        if 'lcd' in object_nearest_corner_dict.keys():
                            quadrant_corner_ids['4'] = object_nearest_corner_dict['lcd']
                        elif 'slider' in object_nearest_corner_dict.keys():
                            quadrant_corner_ids['4'] = object_nearest_corner_dict['slider']
                        else:
                            rospy.logerr('[pose_estimator] Could not locate quadrant 4!')
                            quadrant_corner_ids['4'] = None

                        if quadrant_corner_ids['4'] == None or \
                                quadrant_corner_ids['4'] == quadrant_corner_ids['1'] or \
                                quadrant_corner_ids['4'] == quadrant_corner_ids['2']:
                            rospy.logwarn('[pose_estimator] Using test heuristic to determine quadrant 4...')
                            q1_coordinates = rect_rectified_corners[quadrant_corner_ids['1']]
                            q2_coordinates = rect_rectified_corners[quadrant_corner_ids['2']]
                            corner_distances = np.linalg.norm(rect_rectified_corners - q1_coordinates, axis=1)
                            corner_distances[quadrant_corner_ids['1']] = np.inf
                            corner_distances[quadrant_corner_ids['2']] = np.inf

                            quadrant_corner_ids['4'] = corner_distances.argmin()

                        quadrant_corner_ids['3'] = None
                        corner_quadrant_ids = dict(zip(quadrant_corner_ids.values(), quadrant_corner_ids.keys()))

                        rect_corners = rect_rectified_corners.copy()

                        if debug:
                            print(f'\n[DEBUG] Rectangle Corners:\n{rect_corners}')
                            # print(f'[DEBUG] Quadrant Corner ID Candidates:\n{quadrant_corner_id_candidates}')

                            print(f'\n[DEBUG] Quadrant corner IDs:\n{quadrant_corner_ids}')
                            print(f'[DEBUG] Corner quadrant IDs:\n{corner_quadrant_ids}')

                            print(f'\n[DEBUG] Estimated best-fit plane normal vector:\n{plane_normal_eigh}')

                        # Estimate first orientation vector:
                        if quadrant_corner_ids['1'] is not None and quadrant_corner_ids['2'] is not None:
                            orientation_vector_2 = rect_corners[quadrant_corner_ids['1'], :] - rect_corners[quadrant_corner_ids['2'], :]
                            horizontal_side_found = True
                        elif quadrant_corner_ids['3'] is not None and quadrant_corner_ids['4'] is not None:
                            orientation_vector_2 = rect_corners[quadrant_corner_ids['3'], :] - rect_corners[quadrant_corner_ids['4'], :]
                            horizontal_side_found = True
                        else:
                            rospy.logwarn('[pose_estimator] Can not determine top/bottom of board!')
                            orientation_vector_2 = rect_corners[2, :] - rect_corners[1, :]

                        # Estimate first orientation vector:
                        if quadrant_corner_ids['1'] is not None and quadrant_corner_ids['4'] is not None:
                            orientation_vector_1 = rect_corners[quadrant_corner_ids['1'], :] - rect_corners[quadrant_corner_ids['4'], :]
                            vertical_side_found = True
                        elif quadrant_corner_ids['2'] is not None and quadrant_corner_ids['3'] is not None:
                            orientation_vector_1 = rect_corners[quadrant_corner_ids['2'], :] - rect_corners[quadrant_corner_ids['3'], :]
                            vertical_side_found = True
                        else:
                            rospy.logwarn('[pose_estimator] Can not determine right/left side of board!')
                            orientation_vector_1 = rect_corners[1, :] - rect_corners[0, :]

                        if debug:
                            print(f'\n[DEBUG] Orientation vector 1 (unnormalized):\n{orientation_vector_1}')
                            print(f'[DEBUG] Orientation vector 2 (unnormalized):\n{orientation_vector_2}')

                        orientation_vectors = np.stack((orientation_vector_1 / np.linalg.norm(orientation_vector_1),
                                                        orientation_vector_2 / np.linalg.norm(orientation_vector_2)))

                        tb_orientation_matrix = np.vstack((orientation_vectors, plane_normal_eigh))

                        # Verify that the estimated orientation vectors are both perpendicular to the plane normal vector:
                        if not all((orientation_vectors[:, :] @ plane_normal_eigh) < 1e-10):
                            rospy.logerr('[pose_estimator] Estimated orientation vectors are not orthogonal! Orientation matrix:\n {}'.format(tb_orientation_matrix))
                            rospy.loginfo(f'[pose_estimator] Will re-attempt to estimate orientation...')
                            continue

                        # Re-orient axes for desired convention:
                        reorientation_matrix = np.array([[0, 1, 0], [1, 0, 0], [0, 0, -1]])
                        tb_orientation_matrix = reorientation_matrix @ tb_orientation_matrix

                        tb_tf_matrix = np.hstack((np.vstack((tb_orientation_matrix, np.zeros(3))), np.array([[0, 0, 0, 1]]).T))
                        # tb_tf_matrix = np.hstack((np.vstack((tb_orientation_matrix, np.zeros(3))), np.array([[tb_position[0], tb_position[1], tb_position[2], 1]]).T))

                        # Compute and normalize orientation quaternion:
                        if vertical_side_found and horizontal_side_found:
                            orientation_quaternion = tf_conversions.transformations.quaternion_from_matrix(tb_tf_matrix)
                            orientation_quaternion /= np.linalg.norm(orientation_quaternion)
                        else:
                            rospy.logerr('[pose_estimator] Could not sufficiently ' + \
                                         'determine taskboard orientation.')
                            rospy.logerr('[pose_estimator] Will not publish ' + \
                                         ' {taskboard_frame_name}.')
                            rospy.loginfo(f'[pose_estimator] Will re-attempt to estimate orientation...')
                            continue

                        if debug:
                            print(f'\n[DEBUG] Orientation vector 1 (normalized):\n{orientation_vectors[0, :]}')
                            print(f'[DEBUG] Orientation vector 2 (normalized):\n{orientation_vectors[1, :]}')

                            print(f'\n[DEBUG] Taskboard orientation matrix:\n{tb_orientation_matrix}')
                            print(f'[DEBUG] Taskboard surface tb_position position:\n{tb_position}')

                        elapsed_time = time.time() - tb_orientation_estimation_start_time
                        rospy.loginfo(f'[pose_estimator] Estimated taskboard orientation in {elapsed_time:.2f}s')
                        break

                    else:
                        rospy.logerr(f'[pose_estimator] Failed to estimate TB orientation after {num_retries} attempts.')
                        current_detection_msg_ = None
                        continue

                    elapsed_time = time.time() - position_estimation_start_time
                    rospy.loginfo(f'[pose_estimator] Finished in {elapsed_time:.2f}s')
                    rospy.loginfo(f'[pose_estimator] Continuously publishing current {taskboard_frame_name} transform')

                    ## ----------------------------------------
                    ## Visualizing Results:
                    ## ----------------------------------------

                    if debug:
                        fig = plt.figure('Original Point Cloud Data')
                        ax = fig.add_subplot(projection='3d')
                        ax.scatter(cropped_pc_points_array[:, 0], cropped_pc_points_array[:, 1],
                                   cropped_pc_points_array[:, 2],
                                   label='Original', c='tab:blue', s=0.01, alpha=1.0)
                        ax.legend()
                        ax.set_xlabel('$x$'); ax.set_ylabel('$y$'); ax.set_zlabel('$z$')
                        xlims, ylims, zlims  = ax.get_xlim(), ax.get_ylim(), ax.get_zlim()
                        ax.view_init(-92, -86)

                        fig = plt.figure('Taskboard Orientation Estimation Results')
                        ax = fig.add_subplot(projection='3d')

                        if not hide_pc_points:
                            ax.scatter(tb_points_array[:, 0], tb_points_array[:, 1], tb_points_array[:, 2],
                                       label='Points', c='black', s=0.01, alpha=0.2)

                        # Best-fit plane's normal vector:
                        scale = arrow_scale_factor
                        end_point = plane_normal_eigh * scale
                        ax.arrow3D(*tb_position, *end_point, mutation_scale=10)
                        ax.annotate3D('Plane Normal',
                                      xyz=end_point + tb_position + (scale * .0),
                                      xytext=(0., 0.),
                                      textcoords='offset points',
                                      ha='left', va='bottom')

                        # Visualize best-fit plane representation:
                        plane_bounds = list(zip(tb_points_array.min(axis=(0))[:2],
                                                tb_points_array.max(axis=(0))[:2]))
                        xx, yy = np.meshgrid(np.linspace(plane_bounds[0][0], plane_bounds[0][1], 10),
                                             np.linspace(plane_bounds[1][0], plane_bounds[1][1], 10))
                        A, B, C = plane_normal_eigh
                        D = -tb_position @ plane_normal_eigh
                        zz = (-(A * xx) - (B * yy) - D) / C
                        ax.plot_surface(xx, yy, zz, alpha=0.3, facecolor='grey',
                                        color='black', cstride=3, rstride=3,)
                        ax.annotate3D('Best-fit Plane',
                                      xyz=(plane_bounds[0][1], plane_bounds[1][1], zz.max()),
                                      xytext=(0., 0.), textcoords='offset points',
                                      ha='left', va='bottom')

                        # Visualized fitted and rectified 2D rectangles:
                        if plot_fitted_rectangle:
                            rect_corners_naive = minimum_bounding_rectangle(tb_points_array[:, :2])
                            rect_corners_aug = np.vstack((rect_corners_naive, rect_corners_naive[0, :]))
                            rect_corners_aug = np.hstack((rect_corners_aug,
                                                      np.ones((rect_corners_aug.shape[0], 1)) * tb_position[2]))
                            ax.plot(rect_corners_aug[:, 0], rect_corners_aug[:, 1], rect_corners_aug[:, 2],
                                     color='black', label='Best-Fit Rectangle (Naive)')

                        if plot_fitted_rectified_rectangle:
                            rect_corners_aug = np.vstack((rect_rectified_corners, rect_rectified_corners[0, :]))
                            ax.plot(rect_corners_aug[:, 0], rect_corners_aug[:, 1], rect_corners_aug[:, 2],
                                     color='pink', label='Best-Fit Rectangle (Rectified)')

                        # Annotate quadrant points:
                        for corner_id, quadrant_id in corner_quadrant_ids.items():
                            if corner_id is None:
                                continue

                            corner_point = rect_rectified_corners[corner_id, :]
                            ax.scatter(*corner_point, c='black', marker='s')
                            ha = 'right' if corner_point[0] <= 0 else 'left'
                            va = 'top' if corner_point[1] <= 0 else 'bottom'
                            ax.annotate3D('Q. {}'.format(quadrant_id),
                                          xyz=corner_point + (corner_point * 0.05),
                                          xytext=(0., 0.), textcoords='offset points',
                                          ha=ha, va=va)

                        # Annotate top or bottom side of TB (if possible):
                        if quadrant_corner_ids['1'] is not None and quadrant_corner_ids['2'] is not None:
                            dist = rect_corners[quadrant_corner_ids['1'], :] - rect_corners[quadrant_corner_ids['2'], :]
                            annotation_point = np.array([rect_corners[quadrant_corner_ids['2'], 0] + (dist[0] / 2.),
                                                         rect_corners[quadrant_corner_ids['2'], 1] + (dist[1] / 2.),
                                                         rect_corners[quadrant_corner_ids['2'], 2] + (dist[2] / 2.)])
                            ax.annotate3D('Top',
                                          xyz=annotation_point + (annotation_point * 0.15),
                                          xytext=(0., 0.), textcoords='offset points',
                                          ha='center', va='top')
                        elif quadrant_corner_ids['3'] is not None and quadrant_corner_ids['4'] is not None:
                            dist = rect_corners[quadrant_corner_ids['4'], :] - rect_corners[quadrant_corner_ids['3'], :]
                            annotation_point = np.array([rect_corners[quadrant_corner_ids['3'], 0] + (dist[0] / 2.),
                                                         rect_corners[quadrant_corner_ids['3'], 1] + (dist[1] / 2.),
                                                         rect_corners[quadrant_corner_ids['3'], 2] + (dist[2] / 2.)])
                            ax.annotate3D('Bottom',
                                          xyz=annotation_point + (annotation_point * 0.15),
                                          xytext=(0., 0.), textcoords='offset points',
                                          ha='center', va='top')

                        # Annotate left or right side of TB (if possible):
                        if quadrant_corner_ids['1'] is not None and quadrant_corner_ids['4'] is not None:
                            dist = rect_corners[quadrant_corner_ids['1'], :] - rect_corners[quadrant_corner_ids['4'], :]
                            annotation_point = np.array([rect_corners[quadrant_corner_ids['4'], 0] + (dist[0] / 2.),
                                                         rect_corners[quadrant_corner_ids['4'], 1] + (dist[1] / 2.),
                                                         rect_corners[quadrant_corner_ids['4'], 2] + (dist[2] / 2.)])
                            ax.annotate3D('Right',
                                          xyz=annotation_point + (annotation_point * 0.15),
                                          xytext=(0., 0.), textcoords='offset points',
                                          ha='center', va='top')
                        elif quadrant_corner_ids['2'] is not None and quadrant_corner_ids['3'] is not None:
                            dist = rect_corners[quadrant_corner_ids['2'], :] - rect_corners[quadrant_corner_ids['3'], :]
                            annotation_point = np.array([rect_corners[quadrant_corner_ids['3'], 0] + (dist[0] / 2.),
                                                         rect_corners[quadrant_corner_ids['3'], 1] + (dist[1] / 2.),
                                                         rect_corners[quadrant_corner_ids['3'], 2] + (dist[2] / 2.)])
                            ax.annotate3D('Left',
                                          xyz=annotation_point + (annotation_point * 0.15),
                                          xytext=(0., 0.), textcoords='offset points',
                                          ha='center', va='top')

                        # Visualize orientation vectors:
                        for dim in range(2):
                            scale = arrow_scale_factor
                            end_point = orientation_vectors[dim, :] * scale
                            side_found = horizontal_side_found if dim else vertical_side_found
                            head_width = 0.01 if side_found else 0.
                            arrowstyle = '->' if side_found else '-'

                            ax.arrow3D(*tb_position, *end_point, mutation_scale=10, arrowstyle=arrowstyle)

                        # Visualized detected objects' positions:
                        for object_id, position_array in object_positions_dict.items():
                            ax.scatter(*position_array,
                                       c=[np.array(class_colors_dict[object_id]) / 255.],
                                       label='Object: {}'.format(object_id))

                        ax.legend(prop={'size': 7}, bbox_to_anchor=(1.03, 1.0))
                        ax.set_xlabel('$x$'), ax.set_ylabel('$y$'), ax.set_zlabel('$z$')
                        ax.set_xlim(np.array(xlims) * 1.0)
                        ax.set_ylim(np.array(ylims) * 1.0)
                        ax.set_zlim(np.array(zlims) * 1.0)
                        ax.view_init(-92, -86)

                        plt.show()

                else:
                    rospy.logwarn('[pose_estimator] Could not get a pointcloud message from topic {}!'.format(pointcloud_topic) + \
                                  ' Skipping pose estimation for this detection result...')
                current_detection_msg_ = None

                if object_list_msg is not None:
                    if orientation_quaternion is not None:
                        # Broadcast estimated taskboard frame:
                        tb_quaternion = Quaternion(*orientation_quaternion)

                        tf_msg = TransformStamped()
                        tf_msg.header.stamp = rospy.Time.now()
                        tf_msg.header.frame_id = current_camera_info_msg_.header.frame_id
                        # tf_msg.header.frame_id = 'camera_depth_optical_frame'
                        tf_msg.child_frame_id = taskboard_frame_name
                        tf_msg.transform.translation = Vector3(*tb_position)
                        tf_msg.transform.rotation = tb_quaternion
                        tf_broadcaster.sendTransform(tf_msg)

                        # TODO: Create TF to visualize taskboard_frame wrt dummy_link:
                        # tf_msg_test = TransformStamped()

                        # Re-publish objects list after adding orientations, and
                        # broadcast a frame for each:
                        updated_object_list_msg = ObjectList()

                        object_marker_id_ = 0
                        for object_msg in object_list_msg.objects:
                            label = object_msg.label
                            object_msg.pose.orientation = tb_quaternion
                            updated_object_list_msg.objects.append(object_msg)

                            tf_msg = TransformStamped()
                            tf_msg.header.stamp = rospy.Time.now()
                            tf_msg.header.frame_id = current_camera_info_msg_.header.frame_id
                            # tf_msg.header.frame_id = 'camera_depth_optical_frame'
                            tf_msg.child_frame_id = label + '_frame'
                            tf_msg.transform.translation = Vector3(object_msg.pose.position.x,
                                                                   object_msg.pose.position.y,
                                                                   object_msg.pose.position.z)
                            tf_msg.transform.rotation = tb_quaternion
                            tf_broadcaster.sendTransform(tf_msg)

                        object_poses_publisher.publish(updated_object_list_msg)

                        udp_object_list_msg = updated_object_list_msg
                    else:
                        udp_object_list_msg = object_list_msg

                    # Send object pose data over UDP socket:
                    rospy.loginfo(f'[pose_estimator] Sending object pose data over' + \
                                  f' over UDP...')
                    orientation_success = True if orientation_quaternion is not None else False
                    udp_message = obj_list_msg_to_json(udp_object_list_msg, 
                                                       orientation_success=orientation_success)

                    if debug:
                        print(f'[DEBUG] UDP message: \n{udp_message}')
                    udp_message = udp_message.encode()
                    udp_output_socket.sendto(udp_message, (udp_ip, udp_output_port))

            rate.sleep()
    except (KeyboardInterrupt, rospy.ROSInterruptException):
        rospy.loginfo('[pose_estimator] Stopping node')
