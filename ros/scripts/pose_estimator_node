#!/usr/bin/env python3

"""
Estimates the poses of objects from image bounding boxes (BBs) and depth data.

Note: this component was designed for and tested with an Intel Realsense D435
sensor. In particular, it expects messages to be published on the following 
topics (configurable through launch parameters):
    - /camera/color/image_raw (Image)
    - /camera/depth/color/points (PointCloud2)
    - /camera/color/camera_info  (CameraInfo)
"""

import os
import sys
import time
import copy
import pickle
import socket

import rospy
import rospkg
import tf2_ros
import tf_conversions
import numpy as np
import sensor_msgs.point_cloud2 as pc2

from geometry_msgs.msg import Point, Pose, Quaternion, Vector3, TransformStamped
from sensor_msgs.msg import PointCloud, PointCloud2, CameraInfo
from visualization_msgs.msg import Marker, MarkerArray
from eurobin_perception.msg import BoundingBoxList, ObjectList, Object

from eurobin_perception.pose_estimation import PoseEstimator
from eurobin_perception.utils import bbox_list_msg_to_list, obj_list_msg_to_json
from eurobin_perception.visualization import load_class_color_map


## ----------------------------------------------------------------------
## ROS Callbacks and Message Initializations:
## ----------------------------------------------------------------------

current_pc2_msg_ = None
current_detection_msg_ = None
current_camera_info_msg_ = None

def pc2_callback(msg):
    global current_pc2_msg_ 
    current_pc2_msg_ = msg

def detection_callback(msg):
    global current_detection_msg_ 
    current_detection_msg_ = msg

def camera_info_callback(msg):
    global current_camera_info_msg_ 
    current_camera_info_msg_ = msg

object_marker_publisher_ = None

## ----------------------------------------------------------------------
## Functions:
## ----------------------------------------------------------------------

def get_point_markers(point, frame_id, label='', color_value=(0., 0., 0.)):
    """
    Creates RViz position and text markers for a given 3D point.

    Parameters
    ----------
    point: ndarray
        3D position coordinates
    frame_id: str
        Name of given point's coordinate frame
    label: str
        Text with which the point will be labeled
    color_value: tuple
       (R, G, B) values for the point and text markers [0., 255.] 

    Returns
    -------
    marker_msg: visualization_msgs.Marker 
        Point visualization marker ROS message
    text_marker_msg: visualization_msgs.Marker 
        Text visualization marker ROS message
    """
    global object_marker_id_

    marker_msg = Marker()
    marker_msg.header.frame_id = frame_id
    marker_msg.id = object_marker_id_
    marker_msg.type = 2                                       # Sphere
    marker_msg.action = 0                                     # Add/modify
    marker_msg.pose = Pose(position=Point(*point), 
                           orientation=Quaternion(0., 0., 0., 1.))
    marker_msg.scale.x = 0.01
    marker_msg.scale.y = 0.01
    marker_msg.scale.z = 0.01
    marker_msg.color.r = color_value[0] / 255.
    marker_msg.color.g = color_value[1] / 255.
    marker_msg.color.b = color_value[2] / 255.
    marker_msg.color.a = 1.0
    object_marker_id_ += 1

    text_marker_msg = copy.deepcopy(marker_msg)
    text_marker_msg.id = object_marker_id_
    text_marker_msg.type = 9                                  # Text-view-facing
    text_marker_msg.text = label
    text_marker_msg.pose.position.x = marker_msg.pose.position.x + \
                                      (0.005 * (len(label) / 2.))
    text_marker_msg.pose.position.y = marker_msg.pose.position.y - 0.01
    object_marker_id_ += 1

    return marker_msg, text_marker_msg

def clear_object_markers():
    """
    Clears all current RViz object markers.

    Parameters
    -------
    None

    Returns
    -------
    None
    """
    global object_marker_id_, object_marker_publisher_

    marker_array_msg = MarkerArray()

    marker_msg = Marker()
    marker_msg.id = object_marker_id_ 
    marker_msg.action = 3                                   # Delete all

    marker_array_msg.markers.append(marker_msg)
    object_marker_publisher_.publish(marker_array_msg)

    # Reset object marker ID:
    object_marker_id_ = 0

def get_camera_params_dict():
    """
    Retrieves and returns camera parameters in a dict.

    Parameters
    -------
    None

    Returns
    -------
    camera_params_dict: dict
        Camera intrinsic parameters (P matrix elements)
    """
    global current_camera_info_msg_ 

    f_x = current_camera_info_msg_.P[0]; f_y = current_camera_info_msg_.P[5]
    c_x = current_camera_info_msg_.P[2]; c_y = current_camera_info_msg_.P[6]

    return {'f_x': f_x, 'f_y': f_y, 'c_x': c_x, 'c_y': c_y}


if __name__ == '__main__':
    ## ----------------------------------------------------------------------
    ## ROS Initializations:
    ## ----------------------------------------------------------------------
    rospy.init_node('pose_estimator')
    rate = rospy.Rate(10)

    package_path = rospkg.RosPack().get_path('eurobin_perception')

    class_colors_file_path = rospy.get_param('~class_colors_file_path', '')
    output_dir_path = rospy.get_param('~output_dir_path', 
                                      os.path.join(package_path, 'output_data'))
    taskboard_frame_name = rospy.get_param('~taskboard_frame_name', 'taskboard_frame')
    num_retries = rospy.get_param('~num_retries', 3)
    udp_ip = rospy.get_param('~udp_ip', 'localhost')
    udp_output_port = rospy.get_param('~udp_output_port', 6000)

    pointcloud_topic = rospy.get_param('~pointcloud_topic', '/camera/depth/color/points')
    camera_info_topic = rospy.get_param('~camera_info_topic', '/camera/color/camera_info')
    detector_result_topic = rospy.get_param('~detector_result_topic', '/eurobin_perception/detection_result')
    object_positions_pub_topic = rospy.get_param('~object_positions_pub_topic', '/eurobin_perception/object_positions')
    object_poses_pub_topic = rospy.get_param('~object_poses_pub_topic', '/eurobin_perception/object_poses')
    object_marker_pub_topic = rospy.get_param('~object_marker_pub_topic', '/eurobin_perception/object_markers')
    cropped_pc_pub_topic = rospy.get_param('~cropped_pc_pub_topic', '/eurobin_perception/cropped_pc')

    debug = rospy.get_param('~debug', False)
    save_output = rospy.get_param('~save_output', False)

    pc2_subscriber = rospy.Subscriber(pointcloud_topic, PointCloud2, pc2_callback)
    detector_result_subscriber = rospy.Subscriber(detector_result_topic, BoundingBoxList, detection_callback)
    camera_info_subscriber = rospy.Subscriber(camera_info_topic, CameraInfo, camera_info_callback)

    object_positions_publisher = rospy.Publisher(object_positions_pub_topic, ObjectList, queue_size=10)
    object_poses_publisher = rospy.Publisher(object_poses_pub_topic, ObjectList, queue_size=10)
    object_marker_publisher_ = rospy.Publisher(object_marker_pub_topic, MarkerArray, queue_size=10)
    if debug:
        cropped_pc_debug_publisher = rospy.Publisher(cropped_pc_pub_topic, PointCloud, queue_size=10)

    tf_broadcaster = tf2_ros.TransformBroadcaster()

    ## ----------------------------------------------------------------------
    ## UDP Initializations
    ## ----------------------------------------------------------------------

    rospy.loginfo(f'[pose_estimator] Initializing UDP socket with address' + \
                  f' family AF_INET and type SOCK_DGRAM')
    rospy.loginfo(f'[pose_estimator] Will send messages over IP {udp_ip}' + \
                  f' and port {udp_output_port}.')
    udp_output_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

    ## ----------------------------------------------------------------------
    ## Estimator Execution:
    ## ----------------------------------------------------------------------

    rospy.loginfo('[pose_estimator] Waiting for first camera info message on topic: {}'.format(camera_info_topic))
    while current_camera_info_msg_ is None:
        rospy.sleep(0.1)

        if rospy.is_shutdown():
            rospy.loginfo('[pose_estimator] Stopping node...')
            sys.exit(0)

    class_colors_dict = load_class_color_map(class_colors_file_path)
    orientation_quaternion = None
    object_marker_id_ = 0

    # Initialize PoseEstimator (with current camera parameters):
    position_estimator = PoseEstimator(camera_params_dict=get_camera_params_dict(), class_colors_dict=class_colors_dict)

    rospy.loginfo('[pose_estimator] Will estimate object poses for every message received on topic: {}'.format(detector_result_topic))
    try:
        while not rospy.is_shutdown():
            if current_detection_msg_ is not None:
                clear_object_markers()
                if current_pc2_msg_ is not None:

                    ## ----------------------------------------------------------------------
                    ## Position Estimation:
                    ## ----------------------------------------------------------------------

                    rospy.loginfo('[pose_estimator] Received detection result message.')
                    rospy.loginfo('[pose_estimator] Estimating detected object positions...')
                    position_estimation_start_time = time.time()

                    # Extract list of point positions (x, y, z) from sensor_msgs/PointCloud2 message:
                    read_points_start_time = time.time()
                    pc_point_list = pc2.read_points_list(current_pc2_msg_, 
                                                         skip_nans=True, 
                                                         field_names=("x", "y", "z"))
                    if debug:
                        print(f'[DEBUG] [pose_estimator] Converted PC2 msg to points list in {time.time() - read_points_start_time:.2f}s')
                    # TODO: Consider moving to ros_numpy (partial) vectorization if runtime must be improved.

                    # Get CNN detections and convert to list of dicts:
                    bbox_dict_list = bbox_list_msg_to_list(current_detection_msg_)

                    # Estimate object positions:
                    object_positions_dict, object_points_dict, cropped_pc_points_array = \
                            position_estimator.estimate_object_positions(
                                    bbox_dict_list, pc_point_list, 
                                    cropped_pc_label='taskboard', 
                                    debug=debug
                    )
                    tb_points_array = object_points_dict['taskboard']

                    ## ----------------------------------------
                    ## Publishing Results:
                    ## ----------------------------------------

                    object_list_msg = ObjectList()
                    marker_array_msg = MarkerArray()

                    for label, object_position in object_positions_dict.items():
                        object_msg = Object(label=label, pose=Pose(position=Point(*object_position), 
                                                                   orientation=Quaternion(0., 0., 0., 1.)))
                        object_msg.header.frame_id = current_camera_info_msg_.header.frame_id
                        object_list_msg.objects.append(object_msg)

                        marker_msg, text_marker_msg = get_point_markers(
                                    object_position,
                                    frame_id=current_camera_info_msg_.header.frame_id,
                                    label=object_msg.label,
                                    color_value=class_colors_dict[object_msg.label]
                        )
                        marker_array_msg.markers.append(marker_msg)
                        marker_array_msg.markers.append(text_marker_msg)

                    object_positions_publisher.publish(object_list_msg)
                    object_marker_publisher_.publish(marker_array_msg)

                    if debug:
                        cropped_pc_msg = PointCloud()
                        cropped_pc_msg.header.frame_id = current_camera_info_msg_.header.frame_id
                        cropped_pc_msg.header.stamp = current_pc2_msg_.header.stamp

                        for point in cropped_pc_points_array:
                            cropped_pc_msg.points.append(Point(x=point[0], y=point[1], z=point[2]))
                        cropped_pc_debug_publisher.publish(cropped_pc_msg)

                    elapsed_time = time.time() - position_estimation_start_time
                    rospy.loginfo(f'[pose_estimator] Estimated object positions in {elapsed_time:.2f}s')

                    # Optionally save results: taskboard points and object positions dict.
                    # Mostly for testing and debugging.
                    if save_output:
                        rospy.loginfo(f'[pose_estimator] Saving output data...')
                        if not os.path.isdir(output_dir_path):
                            rospy.loginfo(f'[pose_estimator] Output directory'
                                          f' {output_dir_path} does not exist!'
                                          f' Creating now...')
                            os.makedirs(output_dir_path)

                        with open(os.path.join(output_dir_path, 'pose_estimator_object_positions_dict.pkl'), 'wb') as handle:
                            pickle.dump(object_positions_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)
                        np.save(os.path.join(output_dir_path, 'pose_estimator_taskboard_points.npy'), np.stack(object_points_dict['taskboard']))


                    ## ----------------------------------------------------------------------
                    ## Orientation Estimation:
                    ## ----------------------------------------------------------------------

                    ## ----------------------------------------
                    ## Parameters and Variables:
                    ## ----------------------------------------

                    parameters = {'arrow_scale_factor': 0.2, 
                                  'plot_fitted_rectangle': True, 
                                  'plot_fitted_rectified_rectangle': True, 
                                  'hide_pc_points': False}


                    # Run orientation estimation until successful for a maximum of num_retries times.
                    for attempt_id in range(num_retries):
                        tb_orientation_estimation_start_time = time.time()
                        tb_tf_matrix, orientation_estimation_success, \
                                vertical_side_found, \
                                horizontal_side_found = position_estimator.estimate_tb_orientation(tb_points_array, 
                                                                                                   object_positions_dict, 
                                                                                                   debug=debug,
                                                                                                   **parameters)

                        # Compute and normalize orientation quaternion:
                        if orientation_estimation_success:
                            orientation_quaternion = tf_conversions.transformations.quaternion_from_matrix(tb_tf_matrix)
                            orientation_quaternion /= np.linalg.norm(orientation_quaternion)
                        else:
                            rospy.logerr('[pose_estimator] Could not sufficiently ' + \
                                         'determine taskboard orientation.')
                            rospy.logerr('[pose_estimator] Will not publish ' + \
                                         ' {taskboard_frame_name}.')
                            rospy.loginfo(f'[pose_estimator] Will re-attempt to estimate orientation...')
                            continue

                        elapsed_time = time.time() - tb_orientation_estimation_start_time
                        rospy.loginfo(f'[pose_estimator] Estimated taskboard orientation in {elapsed_time:.2f}s')
                        break

                    else:
                        rospy.logerr(f'[pose_estimator] Failed to estimate TB orientation after {num_retries} attempts.')
                        current_detection_msg_ = None
                        continue

                    elapsed_time = time.time() - position_estimation_start_time
                    rospy.loginfo(f'[pose_estimator] Finished in {elapsed_time:.2f}s')
                    rospy.loginfo(f'[pose_estimator] Continuously publishing current {taskboard_frame_name} transform')

                else:
                    rospy.logwarn('[pose_estimator] Could not get a pointcloud message from topic {}!'.format(pointcloud_topic) + \
                                  ' Skipping pose estimation for this detection result...')
                current_detection_msg_ = None

                if object_list_msg is not None:
                    if orientation_quaternion is not None:
                        # Broadcast estimated taskboard frame:
                        tb_quaternion = Quaternion(*orientation_quaternion)

                        tf_msg = TransformStamped()
                        tf_msg.header.stamp = rospy.Time.now()
                        tf_msg.header.frame_id = current_camera_info_msg_.header.frame_id
                        # tf_msg.header.frame_id = 'camera_depth_optical_frame'
                        tf_msg.child_frame_id = taskboard_frame_name
                        tf_msg.transform.translation = Vector3(*object_positions_dict['taskboard'])
                        tf_msg.transform.rotation = tb_quaternion
                        tf_broadcaster.sendTransform(tf_msg)

                        # TODO: Create TF to visualize taskboard_frame wrt dummy_link:
                        # tf_msg_test = TransformStamped()

                        # Re-publish objects list after adding orientations, and
                        # broadcast a frame for each:
                        updated_object_list_msg = ObjectList()

                        object_marker_id_ = 0
                        for object_msg in object_list_msg.objects:
                            label = object_msg.label
                            object_msg.pose.orientation = tb_quaternion
                            updated_object_list_msg.objects.append(object_msg)

                            tf_msg = TransformStamped()
                            tf_msg.header.stamp = rospy.Time.now()
                            tf_msg.header.frame_id = current_camera_info_msg_.header.frame_id
                            # tf_msg.header.frame_id = 'camera_depth_optical_frame'
                            tf_msg.child_frame_id = label + '_frame'
                            tf_msg.transform.translation = Vector3(object_msg.pose.position.x,
                                                                   object_msg.pose.position.y,
                                                                   object_msg.pose.position.z)
                            tf_msg.transform.rotation = tb_quaternion
                            tf_broadcaster.sendTransform(tf_msg)

                        object_poses_publisher.publish(updated_object_list_msg)

                        udp_object_list_msg = updated_object_list_msg
                    else:
                        udp_object_list_msg = object_list_msg

                    # Send object pose data over UDP socket:
                    rospy.loginfo(f'[pose_estimator] Sending object pose data over' + \
                                  f' over UDP...')
                    orientation_success = True if orientation_quaternion is not None else False
                    udp_message = obj_list_msg_to_json(udp_object_list_msg, 
                                                       orientation_success=orientation_success)

                    if debug:
                        print(f'[DEBUG] UDP message: \n{udp_message}')
                    udp_message = udp_message.encode()
                    udp_output_socket.sendto(udp_message, (udp_ip, udp_output_port))

            rate.sleep()
    except (KeyboardInterrupt, rospy.ROSInterruptException):
        rospy.loginfo('[pose_estimator] Stopping node')
